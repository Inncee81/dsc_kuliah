{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all needed resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json \n",
    "import signal\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theLoop(queue, D, E, filename, seed, old_index, index):\n",
    "    for url in queue.copy():\n",
    "        queue.remove(url)\n",
    "        page1 = requests.get(url)\n",
    "        soup = BeautifulSoup(page1.text, \"html.parser\")\n",
    "        with open(filename+\"/\"+str(old_index)+\".html\", 'x') as f:\n",
    "            f.write(soup.prettify())\n",
    "#         print(url)\n",
    "        links = soup.find_all(\"a\")\n",
    "        for a in links:\n",
    "            if a.has_attr('href') and (a['href'].startswith(seed)) and (a['href'] not in D.values()) and (a['href'] not in queue):\n",
    "                queue.add(a['href'])\n",
    "                index+=1\n",
    "                D[index] = a['href']\n",
    "                E.append(\"{}, {}\".format(old_index, index))\n",
    "        print(len(D))\n",
    "    for k, v in D.items():\n",
    "            old_index = (k if v == list(queue)[k] else old_index)\n",
    "    if len(queue) !=0 :\n",
    "#         print(\"Queue: \", queue)\n",
    "        seed = requests.get(list(queue)[0])\n",
    "        D, E = theLoop(queue, D, E, filename, seed, old_index, index)\n",
    "    else:\n",
    "        return D, E\n",
    "    return D, E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store D and E to a text/json file\n",
    "Function ini untuk menyimpan hasil crawling ke dalam sebuah file txt\n",
    "* file D: kumpulan link dan judul, berupa dictionary (key: url, value: judul halaman) --> dibalik karena ada beberapa page yang memiliki title sama\n",
    "* file E: kumpulan mapping (asal: tujuan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(D, E, filename):\n",
    "    fileD = open(\"DE_\"+filename+\".txt\", \"x\")\n",
    "    fileD.write(json.dumps(D))\n",
    "    fileD.write(json.dumps(E))\n",
    "    fileD.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The \"Simple\" Crawler\n",
    "Mengikuti algoritma Simple Crawler yang dibahas di kelas\n",
    "* Seed URL berupa homepage, diberikan pada saat kita memanggil function crawler\n",
    "* Function 'theLoop' adalah function recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(seed, filename):\n",
    "    page1 = requests.get(seed)\n",
    "    soup = BeautifulSoup(page1.text, \"html.parser\")\n",
    "    index = 1\n",
    "    links = soup.find_all(\"a\")\n",
    "    queue = set()\n",
    "    D = {index : seed}\n",
    "    E = []\n",
    "    old_index = index\n",
    "    for a in links:\n",
    "        if a.has_attr('href') and (seed in a['href']):\n",
    "            queue.add(a['href'])\n",
    "            index+=1\n",
    "            D[index] = a['href']\n",
    "            E.append(\"{}, {}\".format(old_index, index))\n",
    "    D, E = theLoop(queue, D, E, filename, seed, old_index, index)\n",
    "#     print(D.keys())\n",
    "#     print(E)\n",
    "    store_results(D, E, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler: until layer ke N\n",
    "Hanya sedikit memodifikasi basic simple crawler:\n",
    "* memodifikasi bagian pemanggilan function loop\n",
    "* function 'theLoop_n' sedikit berbeda dari 'theLoop': 'theLoop_n' bukan function recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_n(seed, filename):\n",
    "    page1 = requests.get(seed)\n",
    "    soup = BeautifulSoup(page1.text, \"html.parser\")\n",
    "    index = 1\n",
    "    links = soup.find_all(\"a\")\n",
    "    queue = set()\n",
    "    D = {index : seed}\n",
    "    E = []\n",
    "    old_index = index\n",
    "#     layer = int(input(\"Depth: \"))\n",
    "    layer = 3\n",
    "    count = 1\n",
    "    for a in links:\n",
    "        if a.has_attr('href') and (seed in a['href']):\n",
    "            queue.add(a['href'])\n",
    "            index+=1\n",
    "            D[index] = a['href']\n",
    "            E.append(\"{}, {}\".format(old_index, index))\n",
    "#     print(len(D))\n",
    "    while layer>count:\n",
    "        if len(queue) !=0 :\n",
    "#         print(\"Queue: \", queue)\n",
    "            seed = requests.get(list(queue)[0])\n",
    "            D, E, queue, old_index = theLoop_n(queue, D, E, filename, seed, old_index, index)\n",
    "        count+=1\n",
    "#     print(D.keys())\n",
    "#     print(E)\n",
    "    store_results(D, E, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theLoop_n(queue, D, E, filename, seed, old_index, index):\n",
    "    for url in queue.copy():\n",
    "        queue.remove(url)\n",
    "        page1 = requests.get(url)\n",
    "        soup = BeautifulSoup(page1.text, \"html.parser\")\n",
    "        with open(filename+\"/\"+str(old_index)+\".html\", 'x') as f:\n",
    "            f.write(soup.prettify())\n",
    "#         print(url)\n",
    "        links = soup.find_all(\"a\")\n",
    "        for a in links:\n",
    "            if a.has_attr('href') and (a['href'].startswith(seed)) and (a['href'] not in D.values()) and (a['href'] not in queue):\n",
    "                queue.add(a['href'])\n",
    "                index+=1\n",
    "                D[index] = a['href']\n",
    "                E.append(\"{}, {}\".format(old_index, index))\n",
    "#         print(len(D))\n",
    "    for k, v in D.items():\n",
    "        old_index = (k if v == list(queue)[k] else old_index)\n",
    "    return D, E, queue, old_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crawler(\"https://www.uc.ac.id/isb/\", \"ucIsb\") #78\n",
    "# crawler(\"https://kotlinlang.org/\", \"kotlinDocs\") #15\n",
    "# crawler_n(\"https://www.detik.com/\", \"detik\") #116\n",
    "# crawler_n(\"https://pcwonderland.com/\", \"pcwonderland\") #419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler until max time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
